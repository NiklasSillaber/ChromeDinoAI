{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv2\n",
    "from mss import mss\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "import keyboard\n",
    "import time\n",
    "import tqdm as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf                                                               \n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from tensorflow import keras\n",
    "from keras.models import model_from_json, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Following Code Block is the Agent Class. It consists of the Convolutional NN that is the brain of the AI. It also contains the pretrained weights which can be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        #The actual neuronal net consisting of \n",
    "        # - 10 Layers\n",
    "        #    - First Layer: count Neurons = Pixels of Sceenshot\n",
    "        #    - Last Layer: 3 Neurons = 3 States: do nothing, jump, sneak\n",
    "\n",
    "        model = Sequential([ \n",
    "            Conv2D(32, (8,8), input_shape=(76, 384, 4),\n",
    "                   strides=(2,2), activation='relu'),\n",
    "            MaxPooling2D(pool_size=(5,5), strides=(2, 2)),\n",
    "            Conv2D(64, (4,4), activation='relu', strides=(1,1)),\n",
    "            MaxPooling2D(pool_size=(7, 7), strides=(3, 3)),\n",
    "            Conv2D(128, (1, 1), strides=(1,1), activation='relu'),\n",
    "            MaxPooling2D(pool_size=(3,3), strides=(3,3)),\n",
    "            Flatten(),\n",
    "            Dense(384, activation='relu'),  \n",
    "            Dense(64, activation=\"relu\", name=\"layer1\"),\n",
    "            Dense(8, activation=\"relu\", name=\"layer2\"),\n",
    "            Dense(3, activation=\"linear\", name=\"layer3\"),\n",
    "        ])\n",
    "\n",
    "        #pick your learning rate here\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.0001)) \n",
    "        #This is where you import your pretrained weights\n",
    "        model.load_weights(\"DinoGameSpeed4.h5\")\n",
    "        self.model = model\n",
    "        self.memory = []\n",
    "\n",
    "        print(self.model.summary()) \n",
    "        self.xTrain = []\n",
    "        self.yTrain = []\n",
    "        self.loss = []\n",
    "        self.accuracy = []\n",
    "        self.location = 0\n",
    "\n",
    "    def predict(self, state):\n",
    "        stateConv = state\n",
    "        qval = self.model.predict(np.reshape(stateConv, (1, 76, 384, 4)))\n",
    "        return qval\n",
    "\n",
    "    def act(self, state):\n",
    "        qval = self.predict(state)\n",
    "        #you can either pick softmax or epislon greedy actions.\n",
    "        #To pick Softmax, un comment the bottom 2 lines and delete everything below that \n",
    "        # prob = tf.nn.softmax(tf.math.divide((qval.flatten()), 1)) \n",
    "        # action = np.random.choice(range(3), p=np.array(prob))\n",
    "\n",
    "        \n",
    "        #Epsilon-Greedy actions->\n",
    "        z = np.random.random()\n",
    "        epsilon = 0.004\n",
    "        if self.location > 1000:\n",
    "            epsilon = 0.05\n",
    "        epsilon = 0\n",
    "        if z > epsilon:\n",
    "            return np.argmax(qval.flatten())\n",
    "        else:\n",
    "            return np.random.choice(range(3))\n",
    "        \n",
    "        return action\n",
    "\n",
    "    # This function stores experiences in the experience replay\n",
    "    def remember(self, state, nextState, action, reward, done, location):\n",
    "        self.location = location\n",
    "        self.memory.append(np.array([state, nextState, action, reward, done], dtype=object))\n",
    "\n",
    "    #This is where the AI learns\n",
    "    def learn(self):\n",
    "        #Feel free to tweak this. This number is the number of experiences the AI learns from every round\n",
    "        self.batchSize = 256 \n",
    "\n",
    "        #If you don't trim the memory, your GPU might run out of memory during training. \n",
    "        #I found 35000 works well\n",
    "        if len(self.memory) > 35000:\n",
    "            self.memory = []\n",
    "            print(\"trimming memory\")\n",
    "        if len(self.memory) < self.batchSize:\n",
    "            print(\"too little info\")\n",
    "            return  \n",
    "        batch = random.sample(self.memory, self.batchSize)\n",
    "\n",
    "        self.learnBatch(batch)\n",
    "\n",
    "    #The alpha value determines how future oriented the AI is.\n",
    "    #bigger number (up to 1) -> more future oriented\n",
    "    def learnBatch(self, batch, alpha=0.9):\n",
    "        batch = np.array(batch)\n",
    "        actions = batch[:, 2].reshape(self.batchSize).tolist()\n",
    "        rewards = batch[:, 3].reshape(self.batchSize).tolist()\n",
    "\n",
    "        stateToPredict = batch[:, 0].reshape(self.batchSize).tolist()\n",
    "        nextStateToPredict = batch[:, 1].reshape(self.batchSize).tolist()\n",
    "\n",
    "        statePrediction = self.model.predict(np.reshape(\n",
    "            stateToPredict, (self.batchSize, 76, 384, 4)))\n",
    "        nextStatePrediction = self.model.predict(np.reshape(\n",
    "            nextStateToPredict, (self.batchSize, 76, 384, 4)))\n",
    "        statePrediction = np.array(statePrediction)\n",
    "        nextStatePrediction = np.array(nextStatePrediction)\n",
    "\n",
    "        for i in range(self.batchSize):\n",
    "            action = actions[i]\n",
    "            reward = rewards[i]\n",
    "            nextState = nextStatePrediction[i]\n",
    "            qval = statePrediction[i, action]\n",
    "            if reward < -5: \n",
    "                statePrediction[i, action] = reward\n",
    "            else:\n",
    "                #this is the q learning update rule\n",
    "                statePrediction[i, action] += alpha * (reward + 0.95 * np.max(nextState) - qval)\n",
    "\n",
    "        self.xTrain.append(np.reshape(\n",
    "            stateToPredict, (self.batchSize, 76, 384, 4)))\n",
    "        self.yTrain.append(statePrediction)\n",
    "        history = self.model.fit(\n",
    "            self.xTrain, self.yTrain, batch_size=5, epochs=1, verbose=0)\n",
    "        print(type(history))\n",
    "        \n",
    "        loss = history.history.get(\"loss\")[0]\n",
    "        print(\"LOSS: \", loss)\n",
    "        self.loss.append(loss)\n",
    "\n",
    "        accuracy = history.history.get(\"accuracy\")[0]\n",
    "        print(\"LOSS: \", accuracy)\n",
    "        self.accuracy.append(accuracy)\n",
    "        self.xTrain = []\n",
    "        self.yTrain = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have the enviornment classs. This interacts with the actual chrome dino game and gathers screenshots for us. It then analyzes the screenshots and determines when the game is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Enviornment:\n",
    "    def __init__(self):\n",
    "        ########\n",
    "        #these are some various screenshot parameters that I found worked well for different resolutions\n",
    "        #Use it as a starting point but use the open cv code snippets below to tweak your screenshot window\n",
    "        # Do note that the lower the resolution you use, the faster the code runs\n",
    "        # I saw a 200% screenshot framerate increase from dropping my screen resolution from 4k to 720p\n",
    "\n",
    "        #self.mon = {'top': 243, 'left': 0, 'width': 1366, 'height': 270} # 720p resolution\n",
    "        self.mon = {'top': 380, 'left': 0, 'width': 1920, 'height': 380} #1080p resolution\n",
    "        # self.mon = {'top': 1000, 'left': 0, 'width': 3840, 'height': 760} #4k resolution\n",
    "        ########\n",
    "        \n",
    "        self.sct = mss()\n",
    "        self.counter = 0\n",
    "        self.startTime = -1\n",
    "        self.imageBank = []\n",
    "        self.imageBankLength = 4 #number of frames for the conv net\n",
    "        self.actionMemory = 2 #init as 2 to show no action taken   \n",
    "        #image processing\n",
    "        self.ones = np.ones((76,384,4))\n",
    "        self.zeros = np.zeros((76,384,4))  \n",
    "        self.zeros1 = np.zeros((76,384,4))\n",
    "        self.zeros2 = np.zeros((76,384,4))\n",
    "        self.zeros3 = np.zeros((76,384,4))\n",
    "        self.zeros4 = np.zeros((76,384,4))\n",
    "        self.zeros1[:,:,0] = 1\n",
    "        self.zeros2[:,:,1] = 1\n",
    "        self.zeros3[:,:,2] = 1\n",
    "        self.zeros4[:,:,3] = 1\n",
    "\n",
    "    def startGame(self):\n",
    "        #start the game, giving the user a few seconds to click on the chrome tab after starting the code\n",
    "        for i in reversed(range(5)):\n",
    "            print(\"game starting in \", i)\n",
    "            time.sleep(1)\n",
    "\n",
    "    def step(self, action):\n",
    "        actions ={\n",
    "            0: 'space',\n",
    "            1: 'down'\n",
    "        }\n",
    "\n",
    "        if action != self.actionMemory:\n",
    "            if self.actionMemory != 2:\n",
    "                keyboard.release(actions.get(self.actionMemory))\n",
    "            if action != 2:\n",
    "                keyboard.press(actions.get(action))\n",
    "        self.actionMemory = action\n",
    "\n",
    "        #This is where the screenshot happens\n",
    "        screenshot = self.sct.grab(self.mon)\n",
    "        img = np.array(screenshot)[:, :, 0]\n",
    "        processedImg = self._processImg(img)\n",
    "        state = self._imageBankHandler(processedImg)\n",
    "        done = self._done(processedImg)\n",
    "        reward = self._getReward(done)\n",
    "        return state, reward, done\n",
    "\n",
    "    def reset(self):\n",
    "        self.startTime = time.time()\n",
    "        keyboard.press(\"space\")\n",
    "        time.sleep(0.5)\n",
    "        keyboard.release(\"space\")\n",
    "        return self.step(0)\n",
    "\n",
    "    def _processImg(self, img):\n",
    "        img = Image.fromarray(img)\n",
    "        img = img.resize((384, 76), Image.ANTIALIAS)\n",
    "        if np.sum(img) > 2000000:\n",
    "            img = ImageOps.invert(img)\n",
    "        img = self._contrast(img)\n",
    "\n",
    "        #processed screenshot => input of ai\n",
    "        cv2.imshow('eye of the ai', img)\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'): \n",
    "            cv2.destroyAllWindows()\n",
    "        \n",
    "        img = np.reshape(img, (76,384))\n",
    "        return img\n",
    "\n",
    "    def _contrast(self, pixvals):\n",
    "\n",
    "        # minval = np.percentile(pixvals, 2) \n",
    "        # maxval = np.percentile(pixvals, 98) \n",
    "        \n",
    "        minval = 32\n",
    "        maxval = 171\n",
    "        pixvals = np.clip(pixvals, minval, maxval)\n",
    "        pixvals = ((pixvals - minval) / (maxval - minval))\n",
    "        return pixvals\n",
    "\n",
    "    def _imageBankHandler(self, img):\n",
    "        img = np.array(img)\n",
    "        while len(self.imageBank) < (self.imageBankLength): \n",
    "            self.imageBank.append(np.reshape(img,(76,384,1)) * self.ones)\n",
    "\n",
    "        \n",
    "        bank = np.array(self.imageBank)\n",
    "        toReturn = self.zeros\n",
    "        img1 = (np.reshape(img,(76,384,1)) * self.ones)  * self.zeros1\n",
    "        img2 = bank[0] * self.zeros2\n",
    "        img3 = bank[1] * self.zeros3\n",
    "        img4 = bank[2] * self.zeros4\n",
    "\n",
    "\n",
    "        toReturn = np.array(img1 + img2 + img3 + img4)        \n",
    "\n",
    "        self.imageBank.pop(0)\n",
    "        self.imageBank.append(np.reshape(img,(76 ,384,1)) * self.ones)\n",
    "\n",
    "        return toReturn\n",
    "\n",
    "    #Per step you can get an award. If you survive it is 1. If you die it is -15\n",
    "    def _getReward(self,done):\n",
    "        if done:\n",
    "            return -15\n",
    "        else: \n",
    "            return 1\n",
    "    \n",
    "    #check if game over\n",
    "    def _done(self,img):\n",
    "        img = np.array(img)\n",
    "        #get specific part of the screenshot - game over !!!!!chrome -sign\n",
    "        img  = img[20:40, 180:203]\n",
    "\n",
    "        # cv2.imshow(\"image\",img)\n",
    "        # if cv2.waitKey(25) & 0xFF == ord('q'): \n",
    "        #     cv2.destroyAllWindows()\n",
    "\n",
    "        val = np.sum(img)\n",
    "        #Sum of the reset pixels when the game ends in the night mode\n",
    "        expectedVal = 331.9352517985612 \n",
    "        #Sum of the reset pixels when the game ends in the day mode\n",
    "        expectedVal2 = 243.53\n",
    "\n",
    "        # This method checks if the game is done by reading the pixel values\n",
    "        # of the area of the screen at the reset button. Then it compares it to\n",
    "        # a pre determined sum. You might need to fine tune these values since each\n",
    "        # person's viewport will be different. use the following print statements to \n",
    "\n",
    "        # print(\"val: \", val)\n",
    "        # print(\"Difference1: \", np.absolute(val-expectedVal2))\n",
    "        # print(\"Difference2: \", np.absolute(val-expectedVal))\n",
    "        if np.absolute(val-expectedVal) > 15 and np.absolute(val-expectedVal2) > 100:\n",
    "            return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we have the learning loop. Here the Agent/AI is created, the enviornment wrapper is made, and then the AI plays the game. To actually start the game, make sure run this and then click on the chrome dinosaur game. The code gives you a 3 second (adjustable) buffer between the code starting and you shifting to the chrome game. This needs to be done since the AI is not in direct control of the chrome game but rather controlling it via emulating keyboard strokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotX = []\n",
    "\n",
    "agent = Agent() #currently agent is configured with only 2 actions\n",
    "env = Enviornment()\n",
    "env.startGame()\n",
    "\n",
    "episodes = 5   #how often do you want to play\n",
    "learn = True   #do you want to learn\n",
    "\n",
    "for i in tqdm(range(episodes)): \n",
    "    state, reward, doxne = env.reset()\n",
    "    epReward = 0 \n",
    "    done = False\n",
    "    stepCounter = 0\n",
    "    episodeTime = time.time()\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        nextState, reward, done = env.step(action)\n",
    "        ########\n",
    "        #This next section is storing more memory of later parts of the game since \n",
    "        #if you don't do this, most of the experience replay fills up with the \n",
    "        #starting parts of the game since its played more often. A more elegant \n",
    "        #approach to this is \"Prioritized experience replay\" but this is an effective\n",
    "        #alternative too\n",
    "        if stepCounter> 700:\n",
    "            for _ in range(5):\n",
    "                agent.remember(state, nextState, action, reward, done, stepCounter)\n",
    "        elif stepCounter > 40:\n",
    "            agent.remember(state, nextState, action, reward, done, stepCounter)\n",
    "\n",
    "        if done == True: #game ended\n",
    "            for _ in range(10):\n",
    "                agent.remember(state, nextState, action, reward, done, stepCounter)\n",
    "            print(\"breaking\")\n",
    "            break\n",
    "        ########\n",
    "        state = nextState\n",
    "        stepCounter += 1\n",
    "        epReward += reward\n",
    "\n",
    "    #post episode \n",
    "    if stepCounter != 0:\n",
    "        print(\"Avg Frame-Rate: \", 1/((time.time()-episodeTime)/stepCounter))\n",
    "        print(\"Step_counter: \" + str(stepCounter))\n",
    "        print('Reward: ' + str(epReward))\n",
    "\n",
    "    plotX.append(epReward)\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    if learn:\n",
    "        agent.learn()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            agent.model.save_weights (\"DinoGameSpeed5.h5\")\n",
    "            print( \"Saved model\")\n",
    "        \n",
    "\n",
    "#show results\n",
    "plt.title(\"Reward History\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.xticks(range(episodes))\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.plot(range(episodes), plotX) \n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Accuracy History\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.xticks(range(len(agent.accuracy)))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(range(len(agent.accuracy)), agent.accuracy) \n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Loss History\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.xticks(range(len(agent.loss)))\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(range(len(agent.loss)), agent.loss) \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "b0320fee15e209e10f174e2d37ef7cb7bb69409be9e1952836f0aab1874eea1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
